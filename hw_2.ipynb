{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\" returns list of tokens from filename \"\"\"\n",
    "    data = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            for token in line.split(' '):\n",
    "                token = token.rstrip('\\n')\n",
    "                if token != '':\n",
    "                    data.append(token)\n",
    "    return data\n",
    "\n",
    "def get_grams(data):\n",
    "    \"\"\" Gets unigrams and bigrams \"\"\"\n",
    "    bigrams = {}\n",
    "    unigrams = {}\n",
    "    for i in range(len(data)-1):        \n",
    "        if (data[i], data[i+1]) in bigrams:\n",
    "            bigrams[(data[i], data[i + 1])] += 1\n",
    "        else:\n",
    "            bigrams[(data[i], data[i + 1])] = 1\n",
    "        if data[i] in unigrams:\n",
    "            unigrams[data[i]] += 1\n",
    "        else:\n",
    "            unigrams[data[i]] = 1\n",
    "    return unigrams, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bi_probs(unigrams, bigrams):\n",
    "    \"\"\" Uses entire vocab and bigrams to get the probablily of each bigram\"\"\"\n",
    "    bigram_probs = {}\n",
    "    for bigram in bigrams.keys():\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        bigram_probs[bigram] = (bigrams.get(bigram))/(unigrams.get(word1))\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data('data/hw2_training_sets.txt')\n",
    "unigrams, bigrams = get_grams(train_data)\n",
    "bigram_probs = get_bi_probs(unigrams, bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'When': 2,\n",
       "  'trouble': 1,\n",
       "  'or': 1,\n",
       "  'persecution': 1,\n",
       "  'comes': 1,\n",
       "  'because': 1,\n",
       "  'of': 4,\n",
       "  'the': 11,\n",
       "  'word': 1,\n",
       "  ',': 9,\n",
       "  'they': 4,\n",
       "  'quickly': 1,\n",
       "  'fall': 1,\n",
       "  'away': 1,\n",
       "  '.': 4,\n",
       "  'The': 1,\n",
       "  'people': 1,\n",
       "  'were': 2,\n",
       "  'amazed': 1,\n",
       "  'when': 1,\n",
       "  'saw': 2,\n",
       "  'mute': 1,\n",
       "  'speaking': 1,\n",
       "  'crippled': 1,\n",
       "  'made': 1,\n",
       "  'well': 1,\n",
       "  'lame': 1,\n",
       "  'walking': 1,\n",
       "  'and': 3,\n",
       "  'blind': 1,\n",
       "  'seeing': 1,\n",
       "  'Pilate': 1,\n",
       "  'that': 2,\n",
       "  'he': 2,\n",
       "  'was': 3,\n",
       "  'getting': 1,\n",
       "  'nowhere': 1,\n",
       "  'but': 1,\n",
       "  'instead': 1,\n",
       "  'an': 1,\n",
       "  'uproar': 1,\n",
       "  'starting': 1,\n",
       "  'took': 1,\n",
       "  'water': 1,\n",
       "  'washed': 1,\n",
       "  'his': 2,\n",
       "  'hands': 1,\n",
       "  'in': 2,\n",
       "  'front': 1,\n",
       "  'crowd': 1,\n",
       "  'As': 1,\n",
       "  'coming': 1,\n",
       "  'down': 1,\n",
       "  'mountain': 1,\n",
       "  'lord': 1,\n",
       "  'gave': 1,\n",
       "  'them': 1,\n",
       "  'orders': 1,\n",
       "  'not': 1,\n",
       "  'to': 1,\n",
       "  'tell': 1,\n",
       "  'anyone': 1,\n",
       "  'what': 1,\n",
       "  'had': 2,\n",
       "  'seen': 1,\n",
       "  'until': 1,\n",
       "  'son': 1,\n",
       "  'Man': 1,\n",
       "  'risen': 1,\n",
       "  'from': 3,\n",
       "  'dead': 1,\n",
       "  'A': 1,\n",
       "  'certain': 1,\n",
       "  'man': 1,\n",
       "  'Cyrene': 1,\n",
       "  'Simon': 1,\n",
       "  'father': 1,\n",
       "  'Alexander': 1,\n",
       "  'Rufus': 1,\n",
       "  'passing': 1,\n",
       "  'by': 1,\n",
       "  'on': 1,\n",
       "  'way': 1,\n",
       "  'country': 1},\n",
       " {('When', 'trouble'): 1,\n",
       "  ('trouble', 'or'): 1,\n",
       "  ('or', 'persecution'): 1,\n",
       "  ('persecution', 'comes'): 1,\n",
       "  ('comes', 'because'): 1,\n",
       "  ('because', 'of'): 1,\n",
       "  ('of', 'the'): 2,\n",
       "  ('the', 'word'): 1,\n",
       "  ('word', ','): 1,\n",
       "  (',', 'they'): 1,\n",
       "  ('they', 'quickly'): 1,\n",
       "  ('quickly', 'fall'): 1,\n",
       "  ('fall', 'away'): 1,\n",
       "  ('away', '.'): 1,\n",
       "  ('.', 'The'): 1,\n",
       "  ('The', 'people'): 1,\n",
       "  ('people', 'were'): 1,\n",
       "  ('were', 'amazed'): 1,\n",
       "  ('amazed', 'when'): 1,\n",
       "  ('when', 'they'): 1,\n",
       "  ('they', 'saw'): 1,\n",
       "  ('saw', 'the'): 1,\n",
       "  ('the', 'mute'): 1,\n",
       "  ('mute', 'speaking'): 1,\n",
       "  ('speaking', ','): 1,\n",
       "  (',', 'the'): 3,\n",
       "  ('the', 'crippled'): 1,\n",
       "  ('crippled', 'made'): 1,\n",
       "  ('made', 'well'): 1,\n",
       "  ('well', ','): 1,\n",
       "  ('the', 'lame'): 1,\n",
       "  ('lame', 'walking'): 1,\n",
       "  ('walking', 'and'): 1,\n",
       "  ('and', 'the'): 1,\n",
       "  ('the', 'blind'): 1,\n",
       "  ('blind', 'seeing'): 1,\n",
       "  ('seeing', '.'): 1,\n",
       "  ('.', 'When'): 1,\n",
       "  ('When', 'Pilate'): 1,\n",
       "  ('Pilate', 'saw'): 1,\n",
       "  ('saw', 'that'): 1,\n",
       "  ('that', 'he'): 1,\n",
       "  ('he', 'was'): 1,\n",
       "  ('was', 'getting'): 1,\n",
       "  ('getting', 'nowhere'): 1,\n",
       "  ('nowhere', ','): 1,\n",
       "  (',', 'but'): 1,\n",
       "  ('but', 'that'): 1,\n",
       "  ('that', 'instead'): 1,\n",
       "  ('instead', 'an'): 1,\n",
       "  ('an', 'uproar'): 1,\n",
       "  ('uproar', 'was'): 1,\n",
       "  ('was', 'starting'): 1,\n",
       "  ('starting', ','): 1,\n",
       "  (',', 'he'): 1,\n",
       "  ('he', 'took'): 1,\n",
       "  ('took', 'water'): 1,\n",
       "  ('water', 'and'): 1,\n",
       "  ('and', 'washed'): 1,\n",
       "  ('washed', 'his'): 1,\n",
       "  ('his', 'hands'): 1,\n",
       "  ('hands', 'in'): 1,\n",
       "  ('in', 'front'): 1,\n",
       "  ('front', 'of'): 1,\n",
       "  ('the', 'crowd'): 1,\n",
       "  ('crowd', '.'): 1,\n",
       "  ('.', 'As'): 1,\n",
       "  ('As', 'they'): 1,\n",
       "  ('they', 'were'): 1,\n",
       "  ('were', 'coming'): 1,\n",
       "  ('coming', 'down'): 1,\n",
       "  ('down', 'the'): 1,\n",
       "  ('the', 'mountain'): 1,\n",
       "  ('mountain', ','): 1,\n",
       "  (',', 'lord'): 1,\n",
       "  ('lord', 'gave'): 1,\n",
       "  ('gave', 'them'): 1,\n",
       "  ('them', 'orders'): 1,\n",
       "  ('orders', 'not'): 1,\n",
       "  ('not', 'to'): 1,\n",
       "  ('to', 'tell'): 1,\n",
       "  ('tell', 'anyone'): 1,\n",
       "  ('anyone', 'what'): 1,\n",
       "  ('what', 'they'): 1,\n",
       "  ('they', 'had'): 1,\n",
       "  ('had', 'seen'): 1,\n",
       "  ('seen', 'until'): 1,\n",
       "  ('until', 'the'): 1,\n",
       "  ('the', 'son'): 1,\n",
       "  ('son', 'of'): 1,\n",
       "  ('of', 'Man'): 1,\n",
       "  ('Man', 'had'): 1,\n",
       "  ('had', 'risen'): 1,\n",
       "  ('risen', 'from'): 1,\n",
       "  ('from', 'the'): 2,\n",
       "  ('the', 'dead'): 1,\n",
       "  ('dead', '.'): 1,\n",
       "  ('.', 'A'): 1,\n",
       "  ('A', 'certain'): 1,\n",
       "  ('certain', 'man'): 1,\n",
       "  ('man', 'from'): 1,\n",
       "  ('from', 'Cyrene'): 1,\n",
       "  ('Cyrene', ','): 1,\n",
       "  (',', 'Simon'): 1,\n",
       "  ('Simon', ','): 1,\n",
       "  ('the', 'father'): 1,\n",
       "  ('father', 'of'): 1,\n",
       "  ('of', 'Alexander'): 1,\n",
       "  ('Alexander', 'and'): 1,\n",
       "  ('and', 'Rufus'): 1,\n",
       "  ('Rufus', ','): 1,\n",
       "  (',', 'was'): 1,\n",
       "  ('was', 'passing'): 1,\n",
       "  ('passing', 'by'): 1,\n",
       "  ('by', 'on'): 1,\n",
       "  ('on', 'his'): 1,\n",
       "  ('his', 'way'): 1,\n",
       "  ('way', 'in'): 1,\n",
       "  ('in', 'from'): 1,\n",
       "  ('the', 'country'): 1,\n",
       "  ('country', '.'): 1})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_data(\"data/test_set.txt\")\n",
    "get_grams(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7146\n",
      "7146\n",
      "(1, 2)\n",
      "(1203, 115)\n"
     ]
    }
   ],
   "source": [
    "# Let's put it all together. \n",
    "\n",
    "# 1) Load the data, and shuffle the training data.\n",
    "# TODO\n",
    "vocab, train_data = load_data('data/hw2_training_sets.txt')\n",
    "print(len(vocab))\n",
    "# _, dev_data = load_data('data/bible.dev.txt', vocab)\n",
    "# print(len(vocab))\n",
    "_, test_data = load_data('data/test_set.txt', vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "print(train_data[0])\n",
    "random.shuffle(train_data)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183465"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Initialize our model.\n",
    "# TODO\n",
    "\n",
    "our_lm = FeedforwardLM(len(vocab), 10, 15)\n",
    "count_parameters(our_lm)\n",
    "#print_parameters(our_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Perplexity is just exp(2, cross-entropy). So we just use the loss here.\n",
    "def validate(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # TODO: Implement validation function\n",
    "    av_loss = 0\n",
    "    for (x, y) in data[:1000]:\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        y_raw = model(x)\n",
    "        \n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        av_loss += loss\n",
    "\n",
    "    av_loss = av_loss/len(data[:1000])\n",
    "    \n",
    "    print(\"Average loss: \" + str(av_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch: 1 ###\n",
      "Average loss: tensor(6.4516, grad_fn=<DivBackward0>)\n",
      "tensor(5.3513, grad_fn=<DivBackward0>)\n",
      "### Epoch: 2 ###\n",
      "Average loss: tensor(6.5446, grad_fn=<DivBackward0>)\n",
      "tensor(5.1827, grad_fn=<DivBackward0>)\n",
      "### Epoch: 3 ###\n",
      "Average loss: tensor(6.6212, grad_fn=<DivBackward0>)\n",
      "tensor(5.0426, grad_fn=<DivBackward0>)\n",
      "### Epoch: 4 ###\n",
      "Average loss: tensor(6.7176, grad_fn=<DivBackward0>)\n",
      "tensor(4.9312, grad_fn=<DivBackward0>)\n",
      "### Epoch: 5 ###\n",
      "Average loss: tensor(6.8294, grad_fn=<DivBackward0>)\n",
      "tensor(4.8364, grad_fn=<DivBackward0>)\n",
      "### Epoch: 6 ###\n",
      "Average loss: tensor(6.9647, grad_fn=<DivBackward0>)\n",
      "tensor(4.7584, grad_fn=<DivBackward0>)\n",
      "### Epoch: 7 ###\n",
      "Average loss: tensor(7.0675, grad_fn=<DivBackward0>)\n",
      "tensor(4.6981, grad_fn=<DivBackward0>)\n",
      "### Epoch: 8 ###\n",
      "Average loss: tensor(7.2170, grad_fn=<DivBackward0>)\n",
      "tensor(4.6456, grad_fn=<DivBackward0>)\n",
      "### Epoch: 9 ###\n",
      "Average loss: tensor(7.3021, grad_fn=<DivBackward0>)\n",
      "tensor(4.6144, grad_fn=<DivBackward0>)\n",
      "### Epoch: 10 ###\n",
      "Average loss: tensor(7.4390, grad_fn=<DivBackward0>)\n",
      "tensor(4.5831, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 3) Now we train our model. \n",
    "# TODO\n",
    "\n",
    "epochs = 10\n",
    "ce = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = optim.SGD(our_lm.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('### Epoch: ' + str(i+1) + ' ###')\n",
    "    av_loss = 0\n",
    "    our_lm.train()\n",
    "    for (x, y) in train_data[:10000]:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        y_raw = our_lm(x)\n",
    "        y_hat = softmax(y_raw)\n",
    "        \n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        av_loss += loss\n",
    "        \n",
    "        # c) get the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # d) update the weights\n",
    "        optimizer.step()\n",
    "    validate(our_lm, dev_data)\n",
    "    print(av_loss/len(train_data[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict some words!\n",
    "# TODO\n",
    "\n",
    "words = ['walk', 'to']\n",
    "\n",
    "for i in range(10):\n",
    "    word = words[i]\n",
    "    idx = vocab[word]\n",
    "    tensor_idx = torch.tensor(idx)\n",
    "    \n",
    "    raw_output = our_lm(tensor_idx)\n",
    "    probs = softmax(raw_output)\n",
    "    \n",
    "    pred = torch.argmax(probs)\n",
    "    \n",
    "    # Print prediction.\n",
    "    for w, v in vocab.items():\n",
    "        if v == pred:\n",
    "            print(word + ' ' + w)\n",
    "            words.append(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Perplexity is just exp(2, cross-entropy). So we just use the loss here.\n",
    "def validate(model, data):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # TODO: Implement validation function\n",
    "    av_loss = 0\n",
    "    for (x, y) in data[:1000]:\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        y_raw = model(x)\n",
    "        \n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        av_loss += loss\n",
    "\n",
    "    av_loss = av_loss/len(data[:1000])\n",
    "    \n",
    "    print(\"Average loss: \" + str(av_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
