{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First, we will import everything we need. We will also define a couple\n",
    "of useful functions.\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# This is a function that prints the number of trainable parameters \n",
    "# of a model.\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# This functions prints all parameters (and their gradients) of a model.\n",
    "def print_parameters(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name)\n",
    "        print(param.data)\n",
    "        print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now, we define a logistic regression w/o the final sigmoid function = \n",
    "a perceptron without the output threshold.\n",
    "In Python, that's called a linear layer.\n",
    "'''\n",
    "input_units = 6\n",
    "output_units = 1\n",
    "\n",
    "percy = nn.Linear(input_units, output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(percy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3793, -0.2659,  0.2570, -0.1893,  0.1284, -0.0832]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5000, -5.0000, -1.2000,  0.5000,  2.0000,  0.7000]],\n",
      "       requires_grad=True)\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([-0.0718], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(percy.parameters(), lr=1)\n",
    "\n",
    "# We set percy's weights and bias to those from our example.\n",
    "for name, param in percy.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    if name == 'weight':\n",
    "        param.data = torch.tensor([[2.5, -5, -1.2, 0.5, 2.0, 0.7]])\n",
    "    else:\n",
    "        param.data = torch.tensor([0.1])\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8050], grad_fn=<AddBackward0>)\n",
      "tensor([0.6910], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Define an input and compute the output of our classifier for it.\n",
    "x = torch.tensor([3, 2, 1, 3, 0, 4.15])\n",
    "sigi = nn.Sigmoid()\n",
    "\n",
    "y_raw = percy(x)\n",
    "print(y_raw)\n",
    "\n",
    "y_hat = sigi(y_raw) \n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3696, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the cross-entropy loss. \n",
    "y = torch.tensor([1.])\n",
    "\n",
    "ce = nn.BCELoss()  # Binary cross-entropy loss\n",
    "loss = ce(y_hat, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "tensor([[ 2.5000, -5.0000, -1.2000,  0.5000,  2.0000,  0.7000]])\n",
      "tensor([[-0.9269, -0.6179, -0.3090, -0.9269,  0.0000, -1.2822]])\n",
      "bias\n",
      "tensor([0.1000])\n",
      "tensor([-0.3090])\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradient.\n",
    "loss.backward()\n",
    "\n",
    "print_parameters(percy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "tensor([[ 3.4269, -4.3821, -0.8910,  1.4269,  2.0000,  1.9822]])\n",
      "tensor([[-0.9269, -0.6179, -0.3090, -0.9269,  0.0000, -1.2822]])\n",
      "bias\n",
      "tensor([0.4090])\n",
      "tensor([-0.3090])\n"
     ]
    }
   ],
   "source": [
    "# Update our parameters, using the learning rate and the gradient.\n",
    "optimizer.step()\n",
    "\n",
    "# Print percy's parameters again.\n",
    "print_parameters(percy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
